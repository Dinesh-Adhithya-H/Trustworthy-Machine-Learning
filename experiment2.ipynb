{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4FDFa9kw9It"
      },
      "source": [
        "we wish to use the MNIST, construct a feed forward neural network for pruning. such that we remove weights, if the mutual information between those neurons were too low\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RPhQ4vACw9Iy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNf9t1DIw9I1",
        "outputId": "764a5ae6-971e-4898-b741-92b272ea7605"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=False)\n",
        "testloader = DataLoader(datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform), batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the feed-forward neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.input_fc1 = []\n",
        "        self.output_fc1 = []\n",
        "        self.output_fc2 = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        self.input_fc1.extend(x.detach().cpu().numpy())\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        self.output_fc1.extend(x.detach().cpu().numpy())\n",
        "        x = self.fc2(x)\n",
        "        self.output_fc2.extend(F.softmax(x, dim=1).detach().cpu().numpy())\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mutual_information(x, y):\n",
        "    joint_distribution = np.histogram2d(x, y)[0]\n",
        "    marginal_x = np.sum(joint_distribution, axis=0)\n",
        "    marginal_y = np.sum(joint_distribution, axis=1)\n",
        "\n",
        "    joint_distribution /= np.sum(joint_distribution)\n",
        "    marginal_x /= np.sum(marginal_x)\n",
        "    marginal_y /= np.sum(marginal_y)\n",
        "\n",
        "    joint_distribution = joint_distribution.flatten()\n",
        "    marginal_distribution = np.outer(marginal_y, marginal_x).flatten()\n",
        "\n",
        "    # Remove zero entries to avoid division by zero\n",
        "    mask = np.logical_and(joint_distribution > 0, marginal_distribution > 0)\n",
        "\n",
        "    return entropy(joint_distribution[mask], marginal_distribution[mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prune_model(model):\n",
        "    \n",
        "    assert (len(model.input_fc1) == len(model.output_fc1) == len(model.output_fc2))\n",
        "    \n",
        "    print(\"Pruning the model\")\n",
        "    model.input_fc1 = np.array(model.input_fc1)\n",
        "    model.output_fc1 = np.array(model.output_fc1)\n",
        "    model.output_fc2 = np.array(model.output_fc2)\n",
        "    \n",
        "    model_mi = Net()\n",
        "    model_mi.load_state_dict(model.state_dict())\n",
        "    model_mi.input_fc1 = model_mi.output_fc1 = model_mi.output_fc2 = [] # Clear the lists\n",
        "    \n",
        "    # Compute the mutual information between the output of the output layer and the input of the output layer\n",
        "    layer2 = torch.zeros((10,512)) # store the mutual information between the output of the output layer and the input of the output layer\n",
        "    \n",
        "    for i in range(10):\n",
        "        for j in range(512):\n",
        "            v = mutual_information( model.output_fc2[:,i] ,  model.output_fc1[:,j] )\n",
        "            layer2[i][j] = v\n",
        "            \n",
        "    print(layer2.shape)\n",
        "    print(model_mi.fc2.weight.data.shape)\n",
        "    \n",
        "    model_mi.fc2.weight.data[layer2<=0.01]  = 0 # prune the weights of fc2\n",
        "    # make the weights no longer trainable\n",
        "    model_mi.fc2.weight.data[layer2<=0.01].requires_grad = False\n",
        "    \n",
        "    print(\"Finished pruning fc2\")\n",
        "    # find the neurons which are dead\n",
        "    neurons_not_dead = []\n",
        "    count = 0\n",
        "    for i in range(512):\n",
        "        weights_out_neuron = np.abs(model_mi.fc2.weight.data[:,i]).sum()\n",
        "        if weights_out_neuron==0:\n",
        "            count+=1\n",
        "        else:\n",
        "            neurons_not_dead.append(i)\n",
        "    \n",
        "    print(\"Number of neurons dead in fc2: \", count)\n",
        "    \n",
        "    # prune the neurons which are dead\n",
        "    \n",
        "    layer1 = torch.zeros((784,512))\n",
        "    \n",
        "    # pruning the weights of 1st layer\n",
        "    for i in range(784):\n",
        "        if model.input_fc1[:,i].mean()!=-1:\n",
        "            for j in neurons_not_dead:\n",
        "                v = mutual_information( model.output_fc1[:,j] , model.input_fc1[:,i] )\n",
        "                layer1[i][j] = v\n",
        "                \n",
        "                \n",
        "    model_mi.fc1.weight.data[layer1.T<=0.01] = 0 # prune the weights of fc1\n",
        "    model_mi.fc1.weight.data[layer1.T<=0.01].requires_grad = False # make the weights no longer trainable\n",
        "    \n",
        "    print(\"Finished pruning fc1\")\n",
        "    \n",
        "    return model_mi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QH9ZQgK0w9I1"
      },
      "outputs": [],
      "source": [
        "# Train the network\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XefbZYUBw9I2",
        "outputId": "8a27ca45-217a-4623-ab88-86e5b4a423f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruning the model\n",
            "torch.Size([10, 512])\n",
            "torch.Size([10, 512])\n",
            "Finished pruning fc2\n",
            "Number of neurons dead in fc2:  207\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Prune the model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Print the loss and accuracy for this epoch\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[25], line 50\u001b[0m, in \u001b[0;36mprune_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39minput_fc1[:,i]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m neurons_not_dead:\n\u001b[1;32m---> 50\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_information\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_fc1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_fc1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m             layer1[i][j] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m     54\u001b[0m model_mi\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[layer1\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# prune the weights of fc1\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[24], line 2\u001b[0m, in \u001b[0;36mmutual_information\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmutual_information\u001b[39m(x, y):\n\u001b[1;32m----> 2\u001b[0m     joint_distribution \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m     marginal_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(joint_distribution, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m     marginal_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(joint_distribution, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram2d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\twodim_base.py:825\u001b[0m, in \u001b[0;36mhistogram2d\u001b[1;34m(x, y, bins, range, normed, weights, density)\u001b[0m\n\u001b[0;32m    823\u001b[0m     xedges \u001b[38;5;241m=\u001b[39m yedges \u001b[38;5;241m=\u001b[39m asarray(bins)\n\u001b[0;32m    824\u001b[0m     bins \u001b[38;5;241m=\u001b[39m [xedges, yedges]\n\u001b[1;32m--> 825\u001b[0m hist, edges \u001b[38;5;241m=\u001b[39m \u001b[43mhistogramdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist, edges[\u001b[38;5;241m0\u001b[39m], edges[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogramdd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\histograms.py:1050\u001b[0m, in \u001b[0;36mhistogramdd\u001b[1;34m(sample, bins, range, normed, weights, density)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bins[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[1;32m-> 1050\u001b[0m smin, smax \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     n \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(bins[i])\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\histograms.py:321\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[1;34m(a, range)\u001b[0m\n\u001b[0;32m    319\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\core\\_methods.py:44\u001b[0m, in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Set the number of epochs (full passes through the dataset)\n",
        "epochs = 10\n",
        "\n",
        "# Loop over each epoch\n",
        "for e in range(epochs):\n",
        "    # Initialize counters for the number of correct predictions and the total number of predictions\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Loop over each batch of images and labels in the training data\n",
        "    for images, labels in trainloader:\n",
        "        # Zero the gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass the images through the model to get the output probabilities\n",
        "        output = model(images)\n",
        "\n",
        "        # Calculate the loss between the output probabilities and the true labels\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # Backpropagate the gradients of the loss with respect to the model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the total number of predictions\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Update the number of correct predictions\n",
        "        correct += (torch.argmax(output, dim=1) == labels).sum().item()\n",
        "    \n",
        "    # Prune the model\n",
        "    model = prune_model(model)\n",
        "\n",
        "    # Print the loss and accuracy for this epoch\n",
        "    print(f\"Epoch {e+1}/{epochs}, Loss: {loss.item()}\")\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "\n",
        "# Print a message to indicate that training is complete\n",
        "print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4VoBvBrw9JA",
        "outputId": "796c75b2-aea9-49b4-cdb0-2673dba2257c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6762\n"
          ]
        }
      ],
      "source": [
        "# test accuracy of the model with the pruned weights\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in testloader:\n",
        "    output = model(images)\n",
        "    total += labels.size(0)\n",
        "    correct += (torch.argmax(output, dim=1) == labels).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvPb1KKRREOz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
